.. _dictionary:

Словари
=======

В pymorphy2 используются словари из проекта OpenCorpora_,
специальным образом обработанные для быстрых выборок.

.. _OpenCorpora: http://opencorpora.org

Упаковка словаря
----------------

Исходный словарь из OpenCorpora_ представляет собой файл,
в котором слова объединены в "леммы" следующим образом::

    1
    ЁЖ      NOUN,anim,masc sing,nomn
    ЕЖА     NOUN,anim,masc sing,gent
    ЕЖУ     NOUN,anim,masc sing,datv
    ЕЖА     NOUN,anim,masc sing,accs
    ЕЖОМ    NOUN,anim,masc sing,ablt
    ЕЖЕ     NOUN,anim,masc sing,loct
    ЕЖИ     NOUN,anim,masc plur,nomn
    ЕЖЕЙ    NOUN,anim,masc plur,gent
    ЕЖАМ    NOUN,anim,masc plur,datv
    ЕЖЕЙ    NOUN,anim,masc plur,accs
    ЕЖАМИ   NOUN,anim,masc plur,ablt
    ЕЖАХ    NOUN,anim,masc plur,loct

Сначала указывается номер леммы, затем перечисляются формы слова и
соответствующая им грам. информация. Первая форма считается "нормальной".
В словаре около 400тыс. лемм и 5млн отдельных слов.

Если просто загрузить все слова и их грам. информацию в питоний list,
то это займет примерно 2Гб оперативной памяти. Кроме того, эта форма
неудобна для быстрого выполнения операций по анализу и склонению слов.

Упаковка грам. информации
-------------------------

Можно заметить, что набор тегов для слова (``NOUN,anim,masc sing,nomn``)
будет повторяться. Хранить строку целиком для всех 5млн слов накладно
по 2 причинам:

- в питоне не гарантировано, что ``id(string1) == id(string2)``, если
  ``string1 == string2``;
- строки нельзя хранить в array, а у list накладные расходы выше, т.к. он
  реализован как массив указателей (об этом дальше еще будет).

В pymorphy2 все возможные наборы тегов хранятся в массиве; для каждого слова
указывается только номер набора тегов.

Пример::

    1
    ЁЖ      1
    ЕЖА     2
    ЕЖУ     3
    ЕЖА     4
    ЕЖОМ    5
    ЕЖЕ     6
    ЕЖИ     7
    ЕЖЕЙ    8
    ЕЖАМ    9
    ЕЖЕЙ    10
    ЕЖАМИ   11
    ЕЖАХ    12

Связи между леммами
-------------------

В словаре OpenCorpora доступна информация о связях между леммами.
Например, может быть связана лемма для инфинитива и лемма с формами
глагола, соответствующими этому инфинитиву. Или, например, краткое
и полное прилагательное.

Эта информация позволяет склонять слова между частями речи (например,
причастие приводить к глаголу).

В pymorphy2 все связанные леммы просто объединяются в одну большую лемму,
т.к. иначе непонятно, как выделять парадигмы.

Парадигмы
---------

Изначально в словаре из OpenCorpora_ нет понятия "парадигмы" слова.
Парадигма - это таблица форм какого-либо слова, образец для склонения
или спряжения.

.. note::

    В pymorphy2 выделенные явным образом парадигмы слов необходимы для того,
    чтоб склонять неизвестные слова (т.к. при этом нужны образцы для склонения).

    Для других операций явно выделенные парадигмы тоже могут быть удобными,
    хотя все, кроме склонения неизвестных слов, можно было бы выполнять
    достаточно быстро и без явно выделенных парадигм.

Пример исходной леммы::

    375080
    ЧЕЛОВЕКОЛЮБИВ   100
    ЧЕЛОВЕКОЛЮБИВА  102
    ЧЕЛОВЕКОЛЮБИВО  105
    ЧЕЛОВЕКОЛЮБИВЫ  110

У слов есть неизменяемое начало ("стем") и изменяемое
"окончание". Можно было бы выделить парадигму вот так::

    ""      100
    "А"     102
    "О"     105
    "Ы"     110

Этот способ неоптимален, т.к. в словарях OpenCorpora_, например,
у большинства сравнительных прилагательных есть формы на ПО-::

    375081
    ЧЕЛОВЕКОЛЮБИВЕЙ         554
    ЧЕЛОВЕКОЛЮБИВЕЕ         555
    ПОЧЕЛОВЕКОЛЮБИВЕЕ       556
    ПОЧЕЛОВЕКОЛЮБИВЕЙ       557

В этом случае форма слова определяется не только тем, как слово
заканчивается, но и тем, как слово начинается. Если при построении
парадигм учитывать только "стем" и "окончание", то все слово целиком
будет считаться окончанием, а значит каждое сравнительное прилагательное
породит еще одну парадигму. Это увеличит общее количество парадигм в
несколько раз и сделает невозможным склонение несловарных
сравнительных прилагательных, поэтому в pymorphy2 парадигма
определяется как "окончание", "номер грам. информации" и "префикс".

Пример парадигмы для "ЧЕЛОВЕКОЛЮБИВ"::

    ""      100     ""
    "А"     102     ""
    "О"     105     ""
    "Ы"     110     ""

Пример парадигмы для "ЧЕЛОВЕКОЛЮБИВЕЕ"::

    "Й"      554     ""
    "Е"      555     ""
    "Е"      556     "ПО"
    "Й"      557     "ПО"

Окончания и префиксы в парадигмах повторяются, и хорошо
бы их не хранить по многу раз, поэтому все возможные окончания
хранятся в массиве, а в парадигме указывается только номер окончания;
с префиксами то же самое.

Получается примерно так::

    1      554     0
    2      555     0
    2      556     1
    1      557     1

.. note::

    Сейчас все возможные окончания парадигм хранятся в list;
    было бы более эффективно хранить их в DAWG или Trie и
    использовать perfect hash для сопоставления индекс <-> слово,
    но сейчас это не реализовано.

Линеаризация парадигм
---------------------

Тройки "окончание, номер грам. информации, префикс" в tuple хранить
расточительно, т.к. этих троек получается очень много (сотни тысяч),
а каждый tuple требует дополнительной памяти::

    >>> import sys
    >>> sys.getsizeof(tuple())
    56

Поэтому каждая парадигма упаковывается в одномерный массив: сначала идут
все номера окончаний, потом все номера грам. информации, потом все номера
префиксов::

    1 2 2 1 554 555 556 557 0 0 1 1

В этом примере парадигма состояла из 4 форм слова, поэтому в массиве будет
``4*3 = 12`` элементов. Данные можно получить с помощью индексной арифметики:
например, номер грам. информации для формы с индексом 2 (индексация с 0)
будет лежать в элементе массива с номером ``12/3 + 2 = 6``.

Хранить числа в питоньем list накладно, т.к. числа типа int - это
тоже объекты и требуют памяти::

    >>> import sys
    >>> sys.getsizeof(1001)
    24

Память под числа [-5...256] в CPython выделена заранее, но

* это деталь реализации CPython;
* в парадигмах много чисел не из этого интервала;
* list в питоне реализован через массив указателей, а значит требует
  дополнительные 4 или 8 байт на элемент (на 32- и 64-битных системах).

Поэтому данные хранятся в array.array из стандартной библиотеки.

Упаковка слов
-------------

Для хранения данных о словах используется граф (Directed Acyclic Word Graph,
`wiki <http://en.wikipedia.org/wiki/Directed_acyclic_word_graph>`__)
с использованием библиотек DAWG_ (это обертка над C++ библиотекой dawgdic_)
или DAWG-Python_ (это написанная на питоне реализация DAWG, которая не требует
компилятора для установки и работает быстрее DAWG_ под PyPy).

В структуре данных DAWG некоторые общие части слов не
дублируются (=> меньше памяти нужно); кроме того, в DAWG можно быстро
выполнять не только точный поиск слова, но и другие операции - поиск
по префиксу, например.

В pymorphy2 в DAWG помещаются не сами слова, а строки вида

    <СЛОВО> <разделитель> <НОМЕР ПАРАДИГМЫ> <НОМЕР ФОРМЫ В ПАРАДИГМЕ>

Пусть, для примера, у нас есть слова

::

    ДВОР    (3, 1)
    ЁЖ      (4, 1)
    ДВОРНИК (1, 2) и (2, 2)
    ЁЖИК    (1, 2) и (2, 2)

Тогда они будут закодированы в такой граф:

.. digraph:: foo

    rankdir=LR;
    size=9;

    node [shape = doublecircle]; 10 14;
    node [shape = circle];

    0 -> 2 [label=Д];
    0 -> 3 [label=Ё];
    1 -> 4 [label=О];
    2 -> 1 [label=В];
    3 -> 16 [label=Ж];
    4 -> 6 [label=Р];
    5 -> 8 [label=К];
    6 -> 7 [label=Н];
    6 -> 22 [label=sep];
    7 -> 5 [label=И];
    8 -> 9 [label=sep];
    9 -> 12 [label=PARA_1];
    9 -> 15 [label=PARA_2];
    12 -> 10 [label=IND_2];
    13 -> 14 [label=IND_1];
    15 -> 10 [label=IND_2];
    16 -> 32 [label=И];
    16 -> 54 [label=sep];
    17 -> 14 [label=IND_1];
    22 -> 13 [label=PARA_3];
    32 -> 8 [label=К];
    54 -> 17 [label=PARA_4];


Этот подход позволяет экономить память (т.к. данные о парадигмах
и индексах тоже сжимаются в DAWG), ну и алгоритмы упрощаются: например,
для получения всех возможных вариантов разбора слова достаточно найти
все ключи, начинающиеся с

    <СЛОВО> <разделитель>

-- а эта операция (поиск всех ключей по префиксу) в DAWG достаточно эффективная.

.. note::

    На самом деле граф будет немного не такой, т.к. текст кодируется в utf-8,
    а значения в base64, и поэтому узлов будет больше; для получения одной
    буквы или цифры может требоваться совершить несколько переходов.

    Кодировка utf-8 используется из-за того, что кодек utf-8 в питоне
    в несколько раз быстрее однобайтового cp1251. Кодировка цифр в
    base64 - тоже деталь реализации: C++ библиотека, на которой основан DAWG_,
    поддерживает только нуль-терминированные строки. Байт 0 считается
    завершением строки и не может присутствовать в ключе, а для
    двухбайтовых целых числел сложно гарантировать, что оба байта ненулевые.

.. note::

    Подход похож на тот, что описан на `aot.ru <http://aot.ru/>`_.


.. _DAWG: https://github.com/kmike/DAWG
.. _DAWG-Python: https://github.com/kmike/DAWG-Python
.. _dawgdic: https://code.google.com/p/dawgdic/


Итоговый формат данных
----------------------

Таблица с грам. информацией
^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    ['tag1', 'tag2', ...]

``tag<N>`` - набор грам. тегов, например ``NOUN,anim,masc sing,nomn``.

Этот массив занимает где-то 0.5M памяти.

Парадигмы
^^^^^^^^^

::

    paradigms = [
        array.array("<H", [
            suff_id1, .., suff_idN,
            tag_id1, .., tag_idN,
            pref_id1, .., pref_idN
        ]),

        array.array("<H", [
            ...
        ]),

        ...
    ]

    suffixes = ['suffix1', 'suffix2', ...]
    prefixes = ['prefix1', 'prefix2', ...]


``suff_id<N>``, ``tag_id<N>`` и ``pref_id<N>`` - это индексы в таблицах
с возможными окончаниями, грам. информацией и префисками соответственно.

Парадигмы занимают примерно 6-7M памяти.

Слова
^^^^^

Все слова хранятся в ``dawg.RecordDAWG``::

       dawg.RecordDAWG

           'word1': (para_id1, para_index1),
           'word1': (para_id2, para_index2),
           'word2': (para_id1, para_index1),
           ...

В DAWG эти слова занимают примерно 7M памяти.

Алгоритм разбора по словарю
---------------------------

С описанной выше струкрутой словаря разбирать известные слова достаточно
просто. Код на питоне::

    result = []

    # Ищем в DAWG со словами все ключи, которые начинаются
    # с <СЛОВО><sep> (обходом по графу); из этих ключей (из того, что за <sep>)
    # получаем список кортежей [(para_id1, index1), (para_id2, index2), ...].
    #
    # RecordDAWG из библиотек DAWG или DAWG-Python умеет это делать
    # одной командой (с возможностью нечеткого поиска для буквы Ё):

    para_data = self._dictionary.words.similar_items(word, self._ee)

    # fixed_word - это слово с исправленной буквой Ё, для которого был
    # проведен разбор.

    for fixed_word, parse in para_data:
        for para_id, idx in parse:

            # по информации о номере парадигмы и номере слова в
            # парадигме восстанавливаем нормальную форму слова и
            # грамматическую информацию.

            tag = self._build_tag_info(para_id, idx)
            normal_form = self._build_normal_form(para_id, idx, fixed_word)

            result.append(
                (fixed_word, tag, normal_form)
            )

Настоящий код немного отличается, но на алгоритм это не влияет.

Т.к. парадигмы запакованы в линейный массив, требуются дополнительные
шаги для получения данных. Метод ``_build_tag_info`` реализован, например,
вот так::

    def _build_tag_info(self, para_id, idx):

        # получаем массив с данными парадигмы
        paradigm = self._dictionary.paradigms[para_id]

        # индексы грамматической информации начинаются со второй трети
        # массива с парадигмой
        tag_info_offset = len(paradigm) // 3

        # получаем искомый индекс
        tag_id = paradigm[tag_info_offset + tag_id_index]

        # возвращаем соответствующую строку из таблицы с грам. информацией
        return self._dictionary.gramtab[tag_id]

.. note::

    Для разбора слов, которых нет в словаре, в pymorphy2
    есть :ref:`предсказатель <prediction>`.

Формат хранения словаря
-----------------------

Итоговый словарь представляет собой папку с файлами::

    dict/
        meta.json
        gramtab.json
        suffixes.json
        paradigms.array
        words.dawg
        prediction-suffixes.dawg
        prediction-prefixes.dawg

Файлы .json - обычные json-данные; .dawg - это двоичный формат C++ библиотеки
`dawgdic`_; paradigms.array - это массив чисел в двоичном виде.

.. note::

    Если вы вдруг пишете морфологический анализатор не на питоне (и формат
    хранения данных устраивает), то вполне возможно, что будет проще
    использовать эти подготовленные словари, а не конвертировать словари
    из OpenCorpora еще раз; ничего специфичного для питона
    в сконвертированных словарях нет.

Характеристики
--------------

После применения описанных выше методов в pymorphy2 словарь
OpenCorpora занимает около 16Мб оперативной памяти и позволяет проводить
анализ слов со скоростью порядка 50..150 тыс слов/сек. Для сравнения:

* в mystem_ словарь + код занимает около 3Мб оперативной памяти,
  скорость > 100тыс. слов/сек;
* в lemmatizer из aot.ru словарь занимает 9Мб памяти (судя по данным
  `отсюда <http://www.aot.ru/docs/sokirko/Dialog2004.htm>`_),
  скорость > 200тыс слов/сек.;
* в варианте морф. анализатора на конечных автоматах с питоновской оберткой
  к openfst (http://habrahabr.ru/post/109736/) сообщается, что словарь
  занимал 35/3 = 11Мб после сжатия, скорость порядка 2 тыс слов/сек
  без оптимизаций;
* написанный на питоне вариант морф. анализатора на конечных автоматах
  (автор - Konstantin Selivanov) требовал порядка 300Мб памяти, скорость порядка
  2 тыс. слов/сек;
* в `pymorphy 0.5.6`_ полностью загруженный в память словарь
  (этот вариант там не документирован) занимает порядка 300Мб,
  скорость порядка 1-2тыс слов/сек.
* MAnalyzer_ v0.1 (основанный на алгоритмах из pymorphy1, но написанный на C++
  и с использованием dawg) говорят, что скорость разбора 900тыс слов/сек при
  потреблении памяти 40Мб;
* в :ref:`первом варианте <2trie>` формата словарей pymorphy2
  (от которого я отказался) получалась скорость 20-60тыс слов/сек
  при 30M памяти или 2-5 тыс слов/сек при 5Мб памяти.

Цели обогнать C/C++ реализации у pymorphy2 нет; цель - скорость
базового разбора должна быть достаточной для того, чтоб "продвинутые"
операции работали быстро. Мне кажется, 100 тыс. слов/сек или 300 тыс.
слов/сек - это не очень важно для многих задач, т.к. накладные расходы
на обработку и применение результатов разбора все равно, скорее всего,
"съедят" эту разницу (особенно при использовании из питоньего кода).

.. _mystem: http://company.yandex.ru/technologies/mystem/
.. _pymorphy 0.5.6: http://pymorphy.readthedocs.org/en/v0.5.6/index.html
.. _MAnalyzer: https://github.com/Melkogotto/MAnalyzer